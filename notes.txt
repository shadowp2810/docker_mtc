Uses a filesystem called overlay2, where binary is shared by containers.
Docker containers are essentially system processes isolated by "cgroups"

----
Gives increasing verbosity of information
$ docker --version
$ docker version
$ docker info 

Docker Root Dir: /var/lib/docker
is where all the important info(images, layers, containers) is installed.

----
$ sudo apt install tree
$ tree --version
Level 1 
    $ sudo tree -L 1 /var/lib/docker
Level 2
    $ sudo tree -L 2 /var/lib/docker
View images on system
    $ docker images 
Delete all images 
    $ docker rmi -f $ (docker images -a -q)

----
$ docker search python
$ docker search --filter is-official=true python
$ docker help
As we run we can see each layer being installed 
    $ docker pull python
$ docker images 
$ docker images --help
To get image ids of images
    $ docker images -q

----
Pass image id retrieved though sub command into command
    $ docker history (docker images -q)
Format the output and put into file called latest
    $ docker history --format "{{.ID}} {{.CreatedBy}} {{.Size}}" (docker images -q) > latest

----
Many files will be shared when we install another version
    $ docker pull python:3.8
Get the ID we are interested in from 
    $ docker images
$ docker history --format "{{.ID}} {{.CreatedBy}} {{.Size}}" <THE IMAGE ID> > 38
A much smaller build on linux alpine
    $ docker pull python:alpine
Layers are stacked from bottom to top, 
every layer from bottom to top that is the same between two containers
will be shared, but as soon as a layer changes will be rebuilt.

----
$ docker create --help
$ docker create python
Show all processes on system
    $ ps -aux
$ docker ps --help
Where a is all and s is size
    $ docker ps -as
$ docker start --help
$ docker rename sweet_haslett python1

----
Runs the command
    $ docker run python python -c 'print("random sentence")'
Run the interpreter
    $ docker run -it python
$ docker run -it --rm --name python3 python
Run detached
    $ docker run --name python3 -itd python
$ docker exec -it python3 python 
Access shell of container
    $ docker exec -it python3 /bin/sh 
Stops container on exit
    $ docker attach python3

----
$ sudo apt install jq -y
$ docker inspect --help
Where s is size
    $ docker inspect -s python
Pipe to jq 
    $ docker inspect python1 | jq
    $ docker inspect python1 | jq keys
    $ docker inspect python1 | jq .[0] | jq keys
    $ docker inspect python1 | jq .[0].ResolveConfPath
    $ docker inspect python1 | jq -r .[0].ResolveConfPath
    $ sudo cat $(docker inspect python1 | jq -r .[0].ResolveConfPath)

----
Remove all container other than latest
    $ docker rm $(docker ps -a -q -filter before=$(docker ps -a -l --format {{Names}}))
Remove all images
    $ docker rmi -f $(docker images -q)

----
Buysbox is a minimal container that has essential os tools to perform adhoc commands like pinging or renameing.
    $ docer pull buysbox 
To look at where the filesystem layers are located, uses overlay2 
    $ docker inspect busybox 
$ sudo su 
$ cd var/lib/docker/overlay2
$ tree --inodes -L 3
$ df
$ docker run -v /data -d busybox ping google.com
To see if it is running
    $ docker ps
We see a lowerdir, mergeddir, upperdir, workdir
    $ docker inspect $(docker ps -l -q)

----
$ mkdir -p infrastructure/dev1/app
...
Where t is tag and we look for build file in current directory
    $ docker build -t dev1 .
...
$ docker build -t dev1 .
$ docker run -it dev1
...
$ docker build -t dev1 .
$ docker run dev1 app.1.py
Clean up
    $ docker rm -f $($docker ps -a -q)
    $ docker rmi -f $($docker images -q)

----
$ watch docker ps
$ docker run -dp 5000:5000 dev
$ docker pause $(docker ps -l -q)<CONTAINER-ID>
$ docker unpause $(docker ps -l -q)<CONTAINER-ID>
$ docker kill $(docker ps -l -q)<CONTAINER-ID>

----
Take backup of docker image as tar file
$ docker save -o dev.tar dev
$ tar -tvf dev.tar
$ docker load -i dev.tar
$ docker images
$ docker run dev
$ docker tag dev dev:new
  
----
Clean up
    $ docker rm -f $(docker ps -a -q)  
    $ docker rmi -f $(docker images -q)  
$ docker build -t dev1 .
$ docker run -dp 5000:5000 dev1
$ curl localhost:5000 
$ docker exec -it $(docker ps -l -q) /bin/sh
$ docker export --output="test.tar" $(docker ps -l -q)
$ tar -tvf dev.tar | less
$ tar -tvf test.tar | less
$ tar -tvf test.tar | grep app.py
$ docker import test.tar dev:test
$ docker run -dp 5000:5000 dev:test

----
$ docker volume create dev1-vol
$ docker volume ls 
We can find mountpoint directory here
    $ docker volume inspect dev1-vol
$ sudo ls /var/lib/docker/volumes/dev1-vol
$ docker rm -f $(docker ps -a -q)
$ docker rmi -f $(docker images -q)
$ docker build -t dev1 .
$ docker run -dp 5000:5000 --name dev1 --mount source=dev1-vol,target=/app dev1
$ curl localhost:5000 
Now has the contents of app in directory
    $ sudo cd /var/lib/docker/volumes/dev1-vol/_data
$ docker volume rm dev1-vol


-----------------------
$ docker build -t dev1 .
Get rid of container as soons as we exit out of it
    $ docker run --rm --name dev1 dev1
In another termincal window and check if files were ignored. 
    $ docker exec -it dev1 /bin/sh

-----------------------
What we will build:
Node Red is a visual tool used to design IOT flows and other data flows,
we will use it to pull data from python container and send it out to rest of containers,
to automate it we will use a file called "flows.json" and others stored in volume "/data"
Node Red(port 1880) will send a get request to dev1(port 5000) container "GET http://dev1:5000",
which will respond with JSON data,
and that will be sent to Influxdb(port 8086, mounted volume "var/lib/influxdb2"), 
which will be configured with environemnt variables,
we then configure Grafana(port 3000) to read that data and display it,
Influxdb which is also a non relational time series database also has graphing utilities,
but Grafana is more popular for graphing,
withing Grafana we will preprovision our dashboards(dashboards.yml) and data sources(datasources.yml) such as Influxdb,
and also automatically deploy without any additional configuration, using volume "/var/lib/grafana",
Once we run Docker compose up, data will be flowing, stored and displayed. 
We will also deploy Postgres(post 5432), and preprovision that automatically using "schema.sql" and attached volume "/var/lib/postgresql/data"
We will use PostgREST which will infur the schema of the Postgres database and automatically create REST endpoints(POST http://postgrest:3000/temperature_data) for every table on database.
The data will go into Postgres and Grafana will be able to read that data as well and we will have two matching charts,
of all the data that is streaming from dev1 random generator. 

dev1      192.168.128.2:5000
Node-Red  192.168.128.6:1880
Influxdb  192.168.128.3:8086
PostgREST 192.168.128.4:3000
          192.168.160.3:3000
Postgres  192.168.160.2:5432
Grafana   192.168.128.5:3000
          192.168.160.4:3000
          192.168.144.2:3000
          host: 3000

Network CIDRs to reduce attack surface-
Frontend: 192.168.122.0/24
Backend:  192.168.128.0/20
Postgres: 192.168.120.0/24
Host:     172.31.53.0/20

----
In infrastructure directory
    In attached mode
        $ docker-compose up
    In detached mode where it run in background and we can continue using same terminal window
        $ docker-compose up -d 
    $ docker-compose down

----
target is location within container where we want to populate in the volume
$ docker-compose up -d 
$ curl localhost:5000
$ docker ps
$ docker volume ls
$ docker inspect infrastructure_dev1-vol
$ sudo ls /var/lib/docker/volumes/infrastructure_dev1-vol/_data
We can modify the file in volume 
    $ sudo vim /var/lib/docker/volumes/infrastructure_dev1-vol/_data/app.py
    To escape 
    :wq!
We will notice that bringin the contain down does not clear the volume and data
    $ docker-compose down
To remove volume as well
    $ docker-compose down -v

----
Nodered
$ docker-compose up -d 
$ curl localhost:1880
$ docker volume ls
$ docker inspect infrastructure_nodered-vol
$ sudo ls /var/lib/docker/volumes/infrastructure_nodered-vol/_data
 
----
Nodered will pull information and supply to to databases.
Timestamp to http request to msg payload
$ ip addr show | grep 172
Method GET, URL 172.31.62.57:5000, Payload ignore, Return a parsed JSON object
The ip address will change everytime we deploy, so instead we get ipaddress of api
$ docker ps
$ docker inspect dev1 | grep IPAddress
$ docker inspect dev1 | jq .[0] | jq keys
$ docker inspect dev1 | jq .[0].NetworkSettings | jq keys
$ docker inspect dev1 | jq .[0].NetworkSettings.Networks | jq keys
$ docker inspect dev1 | jq .[0].NetworkSettings.Networks[] | jq keys
$ docker inspect dev1 | jq .[0].NetworkSettings.Networks[].IPAddress
Remove quotes
    $ docker inspect dev1 | jq -r .[0].NetworkSettings.Networks[].IPAddress
But since we are using an automation process like Docker, 
we can just replace the <IPAddress>:5000 with api:5000 or dev1:5000

----
$ docker network ls
$ docker inspect infrastructure_default
In another Terminal window
    $ sudo su
    # cd /var/lib/docker/volumes
    # ls
    # cd infrastructure_nodered-vol/
    # ls
    # cd _data
    # ls
    # cat flows.json
$ docker cp nodered:/data/flows.json nodered/
$ docker-compose down -v
$ docker-compose up -d

----
We are going to store the data into Influxdb
Write the following into compose 
    $ docker run -p 8086:8086 \ -v influxdb2:/var/lib/influxdb2 \ influxdb:2.0
$ docker-compose up -d 
$ curl localhost:8086
$ docker ps
$ docker-compose down -v

----
In nodered, install influxdb package, and from influxdb copy over the token in account,
add the token info and other influxdb info to nodered. 
Connect the http request module with function module which will connect to influxdb module. 
Set the function module as 
```
msg.payload = [
    [
        {
            temp: msg.payload.data.temp,
        },
        {
            device: msg.payload.data.device,
            container: msg.payload.data.name
        }
    ]
];
return msg
```

----
To healthcheck, keep running in a window
    $ watch docker compose ps
Shows no healthcheck for influxdb
    $ docker compose up
We make a Dockerfile for influfdb and add a healthcheck.
And in dockercompose we build from dockerfile instead of pulling image.
To make sure it builds, even though it should anyway
    $ docker-compose up --build -d
The watch window should now show a healthcheck from influxdb


----
$ docker cp nodered:/data/flows.json nodered/
$ docker cp nodered:/data/flows_cred.json nodered/
$ docker cp nodered:/data/settings.js nodered/
From the settings.js file, uncomment credentialSecret and set it to false.

----
Login to Postgres from terminal
    $ docker exec -it postgres psql -d pppdb -U pranavp2810 -c "\1"

----
Stop postgres
    $ docker-compose rm -s postgres
$ docker volume ls 
$ docker volume rm infrastructure_postgres-vol
$ docker-compose up --build -d
List tables
    $ docker exec -it postgres psql -d pppdb -U pranavp2810 -c "\dt"
Shoe temperature table
    $ docker exec -it postgres psql -d pppdb -U pranavp2810 -c "select * from temperature_data"

----
We use a dockercompose connection 3000 and not public 4000
    http://postgrest:3000/temperature_data
In nodered we make a connection between the http request node and a new function node and another http request node
In the function node
```
msg.payload = [
    {
        "device":msg.payload.data.device,
        "container":msg.payload.data.container,
        "temperature":msg.payload.data.temp
    }
];
return msg;
```
Copy the flow.json from nodered and save flow.json file in nodered directory. 
----

```
SELECT created_at AS "time", device, avg(temperature) AS "temperature"
FROM temperature_data
WHERE $__timeFilter(created_at)
GROUP BY device, created_at
ORDER BY 1
```

Update the datasources and dashboards file from grafana. 

$ docker-compose down -v
$ docker-compose up --build -d

----
We will now secregrate networks. 
A backend network for flask, node-red. influxdb, grafana and postgrest.
A frontend network for grafana.
A postgres network for postgres, postgrest and grafana. 
A host network for flask, node-red. influxdb, grafana, postgrest and postgres. 

First we remove the ports for dev1, influxdb, postgres and postgrest. 

----
We can use docker exec to backup and restore out postgress database. 
$ docker compose up -d 
$ docker ps 
$ docker compose exec --help
$ docker compose exec postgres /bin/bash
    # exit
We see index starts at 1
    $ docker-compose exec --help
$ docker-compose exec postgres psql -U pranavp2810 -d pppdb 
    To see all tables 
    # /dt
    # select * from temperature_data limit 10
    To quit
    # \q
$ docker-compose exec -T postgres psql -U pranavp2810 -d pppdb -c 'select * from temperature_data limit 10;'
$ docker-compose stop postgrest
Dump a copy of the database into a sql file
    $ docker-compose exec -T postgres pg_dumpall -U pranavp2810 --database pppdb --data-only > dump.sql
Stop container and remove, where s is stop
    $ docker-compose rm -s postgres
$ docker volume ls
$ docker volume rm -f infrastructure_postgres-vol
We create an alias
    $ alias psql="docker-compose exec -T postgres psql"
We bring the container back up
    $ docker-compose up -d postgres
Show an empty database
    $ psql -U pranavp2810 -d pppdb -c 'select * from temperature_data limit 10;'
Pipe it
    $ cat dump.sql | psql -U pranavp2810 -d pppdb
Shows populated database
    $ psql -U pranavp2810 -d pppdb -c 'select * from temperature_data limit 10;'
$ docker-compose start postgrest 

----
